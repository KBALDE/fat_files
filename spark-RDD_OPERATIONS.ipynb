{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ade175d0",
   "metadata": {},
   "source": [
    "# Operation on RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda42f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Utils\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// We have this utils object that we could reuse later on \n",
    "object Utils {\n",
    "  // a regular expression which matches commas but not commas within double quotations\n",
    "  val COMMA_DELIMITER = \",(?=([^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63547137",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Data inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1c5a67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path: String = scala-spark-tutorial/in\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path=\"scala-spark-tutorial/in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46ca4a1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputWords: List[String] = List(spark, hadoop, spark, hive, pig, cassandra, hadoop)\n",
       "wordRdd: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputWords = List(\"spark\", \"hadoop\", \"spark\", \"hive\", \"pig\", \"cassandra\", \"hadoop\")\n",
    "val wordRdd = sc.parallelize(inputWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1131ab0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[String] = Array(spark, hadoop, spark, hive, pig, cassandra, hadoop)\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "600ff60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hadoop\n",
      "spark\n",
      "hive\n",
      "spark\n",
      "pig\n",
      "cassandra\n",
      "hadoop\n"
     ]
    }
   ],
   "source": [
    "// some loop to println\n",
    "for (w <- wordRdd) println(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70c4c149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ward: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at map at <console>:27\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// I can even put it in a variable\n",
    "val ward= for (w <- wordRdd ) yield w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e95b63d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[String] = Array(spark, hadoop, spark, hive, pig, cassandra, hadoop)\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ward.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49da4218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Long = 7\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// we can count rows\n",
    "wordRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec393d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordRddCoutValue: scala.collection.Map[String,Long] = Map(cassandra -> 1, hadoop -> 2, spark -> 2, hive -> 1, pig -> 1)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// now countValue\n",
    "val wordRddCoutValue= wordRdd.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6916691",
   "metadata": {},
   "outputs": [],
   "source": [
    "// We got a Map collection \n",
    "// we better not have much data\n",
    "// Now to print values that are in the resulting countValue RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f471715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(cassandra,1)\n",
      "(hadoop,2)\n",
      "(spark,2)\n",
      "(hive,1)\n",
      "(pig,1)\n"
     ]
    }
   ],
   "source": [
    "for ((k,v) <- wordRddCoutValue) println(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c189243a",
   "metadata": {},
   "source": [
    "## Intersecton of two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "733116bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res41: String = scala-spark-tutorial/in\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1156cad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "julyFirstLogs: org.apache.spark.rdd.RDD[String] = scala-spark-tutorial/in/nasa_19950701.tsv MapPartitionsRDD[16] at textFile at <console>:26\n",
       "augustFirstLogs: org.apache.spark.rdd.RDD[String] = scala-spark-tutorial/in/nasa_19950801.tsv MapPartitionsRDD[18] at textFile at <console>:28\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// read the TSV file \n",
    "val julyFirstLogs = spark.sparkContext.textFile(\"scala-spark-tutorial/in/nasa_19950701.tsv\")\n",
    "\n",
    "val augustFirstLogs = sc.textFile(\"scala-spark-tutorial/in/nasa_19950801.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b897313a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Array[String] = Array(host\tlogname\ttime\tmethod\turl\tresponse\tbytes, \"199.72.81.55\t-\t804571201\tGET\t/history/apollo/\t200\t6245\t\t\")\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "julyFirstLogs.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1cefbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res30: Array[String] = Array(host, 199.72.81.55)\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augustFirstLogs.take(2)\n",
    "julyFirstLogs.map(line => line.split(\"\\t\")(0)).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c58f2c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "julyFirstHosts: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[25] at map at <console>:27\n",
       "augustFirstHosts: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[26] at map at <console>:28\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val julyFirstHosts = julyFirstLogs.map(line => line.split(\"\\t\")(0))\n",
    "val augustFirstHosts = augustFirstLogs.map(line => line.split(\"\\t\")(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ebd35894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intersection: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[32] at intersection at <console>:27\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val intersection = julyFirstHosts.intersection(augustFirstHosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1ccecacd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Array[String] = Array(host, 199.72.81.55, unicomp6.unicomp.net, 199.120.110.21, burger.letters.com, 199.120.110.21, burger.letters.com, burger.letters.com, 205.212.115.106, d104.aa.net, 129.94.144.152, unicomp6.unicomp.net, unicomp6.unicomp.net, unicomp6.unicomp.net, d104.aa.net, d104.aa.net, d104.aa.net, 129.94.144.152, 199.120.110.21, ppptky391.asahi-net.or.jp, net-1-141.eden.com, ppptky391.asahi-net.or.jp, 205.189.154.54, waters-gw.starway.net.au, ppp-mia-30.shadow.net, 205.189.154.54, alyssa.prodigy.com, ppp-mia-30.shadow.net, dial22.lloyd.com, smyth-pc.moorecap.com, 205.189.154.54, ix-orl2-01.ix.netcom.com, ppp-mia-30.shadow.net, ppp-mia-30.shadow.net, 205.189.154.54, ppp-mia-30.shadow.net, ppp-mia-30.shadow.net, ix-orl2-01.ix.netcom.com, gayle-gaston.tenet.edu, piweba3y.pro...\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "julyFirstHosts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e778d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res32: Array[String] = Array(alyssa.prodigy.com, www-d1.proxy.aol.com, piweba4y.prodigy.com, piweba2y.prodigy.com, www-b3.proxy.aol.com, columbia.acc.brad.ac.uk, host, spectrum.xerox.com, beglinger.dial-up.bdt.com, www-d3.proxy.aol.com, freenet.edmonton.ab.ca, dd08-021.compuserve.com, netcom3.netcom.com, www-b5.proxy.aol.com, disarray.demon.co.uk, ottgate2.bnr.ca, www-a2.proxy.aol.com, pm206-52.smartlink.net, vagrant.vf.mmc.com, www-a1.proxy.aol.com, alpha2.csd.uwm.edu, piweba1y.prodigy.com, srv1.freenet.calgary.ab.ca, hitiij.hitachi.co.jp, ccn.cs.dal.ca, wwwproxy.info.au, www-d2.proxy.aol.com, server.elysian.net, hella.stm.it, piweba3y.prodigy.com, ntigate.nt.com, www-b2.proxy.aol.com, palona1.cns.hp.com, www-d4.proxy.aol.com, bettong.client.uq.oz.au, koala.melbpc.org.au, magicall.daco...\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7f0f327",
   "metadata": {},
   "outputs": [],
   "source": [
    "//intersection.filter(host => host !=\"host\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34df76e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanedHostIntersection: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[38] at filter at <console>:26\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cleanedHostIntersection = intersection.filter(host => host != \"host\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9747190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now We can save it !!!\n",
    "//cleanedHostIntersection.saveAsTextFile(\"out/nasa_logs_same_hosts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2b630",
   "metadata": {},
   "source": [
    "## Union RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e6c4b071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregatedLogLines: org.apache.spark.rdd.RDD[String] = UnionRDD[39] at union at <console>:28\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// working on the same RDDs\n",
    "val aggregatedLogLines = julyFirstLogs.union(augustFirstLogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e1f5f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: Long = 20000\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregatedLogLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "85dbb353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res43: Long = 10000\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "julyFirstLogs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a337f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res44: Long = 10000\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augustFirstLogs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6db5b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "// It looks like an append !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40392614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res46: Array[String] = Array(host\tlogname\ttime\tmethod\turl\tresponse\tbytes, \"199.72.81.55\t-\t804571201\tGET\t/history/apollo/\t200\t6245\t\t\", \"unicomp6.unicomp.net\t-\t804571206\tGET\t/shuttle/countdown/\t200\t3985\t\t\", \"199.120.110.21\t-\t804571209\tGET\t/shuttle/missions/sts-73/mission-sts-73.html\t200\t4085\t\t\", \"burger.letters.com\t-\t804571211\tGET\t/shuttle/countdown/liftoff.html\t304\t0\t\t\", \"199.120.110.21\t-\t804571211\tGET\t/shuttle/missions/sts-73/sts-73-patch-small.gif\t200\t4179\t\t\", \"burger.letters.com\t-\t804571212\tGET\t/images/NASA-logosmall.gif\t304\t0\t\t\", \"burger.letters.com\t-\t804571212\tGET\t/shuttle/countdown/video/livevideo.gif\t200\t0\t\t\", \"205.212.115.106\t-\t804571212\tGET\t/shuttle/countdown/countdown.html\t200\t3985\t\t\", \"d104.aa.net\t-\t804571213\tGET\t/shuttle/countdown/\t200\t3985\t\t\", \"129.94.144.152\t-\t804571213\tGET...\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregatedLogLines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52717ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isNotHeader: (line: String)Boolean\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isNotHeader(line: String): Boolean = !(line.startsWith(\"host\") && line.contains(\"bytes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e844c476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanLogLines: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[40] at filter at <console>:28\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cleanLogLines = aggregatedLogLines.filter(line => isNotHeader(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8fdecda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res48: Long = 19998\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanLogLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b375d1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res49: String = \"199.72.81.55\t-\t804571201\tGET\t/history/apollo/\t200\t6245\t\t\"\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanLogLines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "df39e5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample: org.apache.spark.rdd.RDD[String] = PartitionwiseSampledRDD[41] at sample at <console>:26\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sample = cleanLogLines.sample(withReplacement = true, fraction = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7675fb57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res50: Long = 1976\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0e6cf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res59: Double = 0.09880988098809881\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//1 - \n",
    "sample.count()*1.0/cleanLogLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a5a8788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Now I can save it\n",
    "// sample.saveAsTextFile(\"out/sample_nasa_logs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a2b6f",
   "metadata": {},
   "source": [
    "# PERSIST RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b624d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputIntegers: List[Int] = List(1, 2, 3, 4, 5)\n",
       "integerRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[42] at parallelize at <console>:26\n",
       "res62: integerRdd.type = ParallelCollectionRDD[42] at parallelize at <console>:26\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputIntegers = List(1, 2, 3, 4, 5)\n",
    "val integerRdd = sc.parallelize(inputIntegers)\n",
    "\n",
    "integerRdd.persist()\n",
    "//(StorageLevel.MEMORY_ONLY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8816d267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res63: Array[Int] = Array(1, 2, 3, 4, 5)\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integerRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8844c040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reduceRdd: Int = 120\n",
       "res66: Long = 5\n"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Reduce\n",
    "val reduceRdd = integerRdd.reduce((x, y) => x * y)\n",
    "integerRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b39cda1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res68: Int = 120\n"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduceRdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741bb84e",
   "metadata": {},
   "source": [
    "# Sum of Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "51d1d3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = scala-spark-tutorial/in/prime_nums.text MapPartitionsRDD[44] at textFile at <console>:25\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"scala-spark-tutorial/in/prime_nums.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3e62fb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res70: Array[String] = Array(\"  2\t  3\t  5\t  7\t 11\t 13\t 17\t 19\t 23\t 29\", \" 31\t 37\t 41\t 43\t 47\t 53\t 59\t 61\t 67\t 71\", \" 73\t 79\t 83\t 89\t 97\t101\t103\t107\t109\t113\", 127\t131\t137\t139\t149\t151\t157\t163\t167\t173, 179\t181\t191\t193\t197\t199\t211\t223\t227\t229, 233\t239\t241\t251\t257\t263\t269\t271\t277\t281, 283\t293\t307\t311\t313\t317\t331\t337\t347\t349, 353\t359\t367\t373\t379\t383\t389\t397\t401\t409, 419\t421\t431\t433\t439\t443\t449\t457\t461\t463, 467\t479\t487\t491\t499\t503\t509\t521\t523\t541)\n"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f1844699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res71: Long = 10\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e32c0600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numbers: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[45] at flatMap at <console>:26\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numbers = lines.flatMap(line => line.split(\"\\\\s+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5f72bd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res72: Array[String] = Array(\"\", 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, \"\", 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, \"\", 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541)\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "461eb915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validNumbers: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[46] at filter at <console>:26\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val validNumbers = numbers.filter(number => !number.isEmpty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "78ec08a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res73: Array[String] = Array(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541)\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validNumbers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ad54deac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intNumbers: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[47] at map at <console>:26\n"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val intNumbers = validNumbers.map(number => number.toInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "18f7e781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res74: Array[Int] = Array(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137, 139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199, 211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277, 281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439, 443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521, 523, 541)\n"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intNumbers.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7cd971d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum is: 24133\n"
     ]
    }
   ],
   "source": [
    "println(\"Sum is: \" + intNumbers.reduce((x, y) => x + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "49bd3ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res77: Long = 100\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intNumbers.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a06b7",
   "metadata": {},
   "source": [
    "# Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "11c297fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lines: org.apache.spark.rdd.RDD[String] = scala-spark-tutorial/in/word_count.text MapPartitionsRDD[49] at textFile at <console>:27\n",
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[50] at flatMap at <console>:28\n"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lines = sc.textFile(\"scala-spark-tutorial/in/word_count.text\")\n",
    "val words = lines.flatMap(line => line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d2de2ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res79: Array[String] = Array(The history of New York begins around 10,000 BC, when the first Native Americans arrived. By 1100 AD, New York's main native cultures, the Iroquoian and Algonquian, had developed. European discovery of New York was led by the French in 1524 and the first land claim came in 1609 by the Dutch. As part of New Netherland, the colony was important in the fur trade and eventually became an agricultural resource thanks to the patroon system. In 1626 the Dutch bought the island of Manhattan from Native Americans.[1] In 1664, England renamed the colony New York, after the Duke of York (later James II & VII.) New York City gained prominence in the 18th century as a major trading port in the Thirteen Colonies., \"\", New York played a pivotal role during the American Rev...\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d5439d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res78: Array[String] = Array(The, history, of, New, York, begins, around, 10,000, BC,, when, the, first, Native, Americans, arrived., By, 1100, AD,, New, York's, main, native, cultures,, the, Iroquoian, and, Algonquian,, had, developed., European, discovery, of, New, York, was, led, by, the, French, in, 1524, and, the, first, land, claim, came, in, 1609, by, the, Dutch., As, part, of, New, Netherland,, the, colony, was, important, in, the, fur, trade, and, eventually, became, an, agricultural, resource, thanks, to, the, patroon, system., In, 1626, the, Dutch, bought, the, island, of, Manhattan, from, Native, Americans.[1], In, 1664,, England, renamed, the, colony, New, York,, after, the, Duke, of, York, (later, James, II, &, VII.), New, York, City, gained, prominence, in, the, 18th, cen...\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "40aa6cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCounts: scala.collection.Map[String,Long] = Map(Twenties, -> 1, II -> 2, industries. -> 1, economy -> 1, \"\" -> 7, ties -> 2, buildings -> 1, for -> 3, eleventh -> 1, ultimately -> 1, support -> 1, channels -> 1, Thereafter, -> 1, subsequent -> 1, defense -> 1, series -> 1, proposed -> 1, any -> 1, 1790, -> 1, city -> 1, war. -> 2, southern -> 2, across -> 1, operations -> 1, 18th -> 1, challenge -> 1, in -> 21, Park -> 1, expressed -> 1, Civil -> 1, point -> 2, cultural -> 1, 1777, -> 1, claim -> 1, labor -> 1, British -> 3, influenced -> 1, War -> 2, representatives -> 1, patroon -> 1, system -> 1, Iroquoian -> 1, Battery -> 1, nationally -> 1, 1664, -> 1, history -> 1, killing -> 1, late -> 1, renewed -> 1, City's -> 1, shrank. -> 1, After -> 1, Wall -> 1, In -> 3, state -> 5, 11 ...\n"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCounts = words.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1ae616fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "//for ((word, count) <- wordCounts) println(word + \" : \" + count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5c3190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
