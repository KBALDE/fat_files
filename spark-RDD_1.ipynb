{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da92bcf",
   "metadata": {},
   "source": [
    "We might need to use the Spark’s lower-level APIs, specifically the Resilient Distributed Dataset (RDD), the SparkContext, and distributed shared variables like accumulators and broadcast variables.\n",
    "\n",
    "\n",
    "There are two sets of low-level APIs: there is one for manipulating distributed data (RDDs), and another for distributing and manipulating distributed shared variables (broadcast variables and accumulators).\n",
    "\n",
    "\n",
    "A SparkContext is the entry point for low-level API functionality. You access it through the SparkSession\n",
    "\n",
    "\n",
    "### Datasets and RDDs of Case Classes\n",
    "We noticed this question on the web and found it to be an interesting one: what is the difference between RDDs of Case Classes and Datasets? The difference is that Datasets can still take advantage of the wealth of functions and optimizations that the Structured APIs have to offer. With Datasets, you do not need to choose between only operating on JVM types or on Spark types, you can choose whatever is either easiest to do or most flexible. You get the both of best worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "574088d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://kindis-mbp:4044\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1646844762948)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@4a5f82ff\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a1d1f",
   "metadata": {},
   "source": [
    "## Creating an RDD\n",
    "One of the easiest ways to get RDDs is from an existing DataFrame or Dataset. Converting these to an RDD is simple: just use the `rdd` method on any of these data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e8dcbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "valRdd: org.apache.spark.rdd.RDD[Long] = MapPartitionsRDD[11] at rdd at <console>:25\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala: converts a Dataset[Long] to RDD[Long]\n",
    "val valRdd = spark.range(500).rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a3dd1",
   "metadata": {},
   "source": [
    "An RDD of type Row is a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "552cc8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[org.apache.spark.sql.Row] = Array([0], [1], [2], [3], [4], [5], [6], [7], [8], [9])\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).toDF().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1563f5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d: org.apache.spark.sql.DataFrame = [id: bigint]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val d= spark.range(10).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "818d3d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[(String, String)] = Array((id,LongType))\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdcddd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "// in Scala\n",
    "//spark.range(10).toDF().rdd.map(rowObject => rowObject.getLong(0)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c93400",
   "metadata": {},
   "source": [
    "### From a Local Collection\n",
    "To create an RDD from a collection, you will need to use the `parallelize` method on a `SparkContext` (within a `SparkSession`). This turns a single node collection into a parallel collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd056254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myCollection: Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple,, yeah, that, is, the, truth, about, Data)\n",
       "words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[52] at parallelize at <console>:30\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple, yeah that is the truth about Data\"\n",
    "  .split(\" \")\n",
    "\n",
    "val words = spark.sparkContext.parallelize(myCollection, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0a5479f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res28: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[52] at parallelize at <console>:30\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d9cc98",
   "metadata": {},
   "source": [
    "An additional feature is that you can then name this RDD to show up in the Spark UI according to a given name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a38f90c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res29: String = myWords\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "words.setName(\"myWords\")\n",
    "words.name // myWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadee100",
   "metadata": {},
   "source": [
    "### From Data Sources\n",
    "RDDs do not have a notion of “Data Source APIs” like DataFrames do; they primarily define their dependency structures and lists of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e11238a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[String] = scala-spark-tutorial/in/airports.text MapPartitionsRDD[36] at textFile at <console>:24\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd=spark.sparkContext.textFile(\"scala-spark-tutorial/in/airports.text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f6e1cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: Array[String] = Array(1,\"Goroka\",\"Goroka\",\"Papua New Guinea\",\"GKA\",\"AYGA\",-6.081689,145.391881,5282,10,\"U\",\"Pacific/Port_Moresby\", 2,\"Madang\",\"Madang\",\"Papua New Guinea\",\"MAG\",\"AYMD\",-5.207083,145.7887,20,10,\"U\",\"Pacific/Port_Moresby\", 3,\"Mount Hagen\",\"Mount Hagen\",\"Papua New Guinea\",\"HGU\",\"AYMH\",-5.826789,144.295861,5388,10,\"U\",\"Pacific/Port_Moresby\", 4,\"Nadzab\",\"Nadzab\",\"Papua New Guinea\",\"LAE\",\"AYNZ\",-6.569828,146.726242,239,10,\"U\",\"Pacific/Port_Moresby\", 5,\"Port Moresby Jacksons Intl\",\"Port Moresby\",\"Papua New Guinea\",\"POM\",\"AYPY\",-9.443383,147.22005,146,10,\"U\",\"Pacific/Port_Moresby\", 6,\"Wewak Intl\",\"Wewak\",\"Papua New Guinea\",\"WWK\",\"AYWK\",-3.583828,143.669186,19,10,\"U\",\"Pacific/Port_Moresby\", 7,\"Narsarsuaq\",\"Narssarssuaq\",\"Greenland\",\"UAK\",\"BGBW\",61.160517,-45.425978,112,-3,\"...\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "434b2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "// alternative to read each file as a records\n",
    "//spark.sparkContext.wholeTextFiles(\"/some/path/withTextFiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0608357",
   "metadata": {},
   "source": [
    "## Manipulating RDDs\n",
    "You manipulate RDDs in much the same way that you manipulate DataFrames. As mentioned, the core difference being that you manipulate `raw Java` or `Scala objects` instead of `Spark types`\n",
    "\n",
    "\n",
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f76621",
   "metadata": {},
   "source": [
    "#### distinct\n",
    "A distinct method call on an RDD removes duplicates from the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d652d2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res30: Long = 16\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(words.count())\n",
    "words.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0f262",
   "metadata": {},
   "source": [
    "### filter\n",
    "Filtering is equivalent to creating a SQL-like where clause. You can look through our records in the RDD and see which ones match some predicate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b05a267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "startsWithS: (individual: String)Boolean\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "def startsWithS(individual:String) = {\n",
    "  individual.startsWith(\"S\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c968868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res32: Array[String] = Array(Spark, Simple,)\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.filter(startsWithS).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a12f55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Array[String] = Array(Spark, Simple,)\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "words.filter(word => startsWithS(word)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae8dbd",
   "metadata": {},
   "source": [
    "### Map\n",
    "You specify a function that returns the value that you want, given the correct input. You then apply that, record by record. Let’s perform something similar to what we just did. In this example, we’ll map the current word to the word, its starting letter, and whether the word begins with “S.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33395ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3203240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words2: org.apache.spark.rdd.RDD[(String, Char, Boolean)] = MapPartitionsRDD[59] at map at <console>:27\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val words2 = words.map(word => (word, word(0), word.startsWith(\"S\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fbe32b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: Array[(String, Char, Boolean)] = Array((Spark,S,true), (The,T,false), (Definitive,D,false), (Guide,G,false), (:,:,false), (Big,B,false), (Data,D,false), (Processing,P,false), (Made,M,false), (Simple,,S,true), (yeah,y,false), (that,t,false), (is,i,false), (the,t,false), (truth,t,false), (about,a,false), (Data,D,false))\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e66f931d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res35: Array[(String, Char, Boolean)] = Array((Spark,S,true), (Simple,,S,true))\n"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// filter relevant boolean values\n",
    "// in Scala\n",
    "words2.filter(record => record._3).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888cb406",
   "metadata": {},
   "source": [
    "### flatMap\n",
    "flatMap provides a simple extension of the `map` function we just looked at. Sometimes, each current row should return multiple rows, instead. \n",
    "\n",
    "For example, you might want to take your set of words and `flatMap` it into a set of characters. Because each word has multiple characters, you should use flatMap to expand it. flatMap requires that the ouput of the map function be an iterable that can be expanded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67201b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res40: Array[String] = Array(Spark, The, Definitive, Guide, :, Big, Data, Processing, Made, Simple,, yeah, that, is, the, truth, about, Data)\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "047e5e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: Array[Char] = Array(S, p, a, r, k, T, h, e, D, e)\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "words.flatMap(word => word.toSeq).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa249165",
   "metadata": {},
   "source": [
    "### Random Splits\n",
    "We can also randomly split an RDD into an Array of RDDs by using the randomSplit method, which accepts an Array of weights and a random seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "358747b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fiftyFiftySplit: Array[org.apache.spark.rdd.RDD[String]] = Array(MapPartitionsRDD[67] at randomSplit at <console>:27, MapPartitionsRDD[68] at randomSplit at <console>:27)\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "val fiftyFiftySplit = words.randomSplit(Array[Double](0.5, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cb7dd5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res48: Array[org.apache.spark.rdd.RDD[String]] = Array(MapPartitionsRDD[67] at randomSplit at <console>:27, MapPartitionsRDD[68] at randomSplit at <console>:27)\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fiftyFiftySplit.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44441e2",
   "metadata": {},
   "source": [
    "## Actions\n",
    "Just as we do with DataFrames and Datasets, we specify actions to kick off our specified transformations. \n",
    "\n",
    "Actions either collect data to the `driver` or `write` to an external data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97101c9c",
   "metadata": {},
   "source": [
    "### reduce\n",
    "You can use the reduce method to specify a function to “reduce” an RDD of any kind of value to one value. For instance, given a set of numbers, you can reduce this to its sum by specifying a function that takes as input two values and reduces them into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe621ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res49: Int = 210\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "spark.sparkContext.parallelize(1 to 20).reduce(_ + _) // 210"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e59d63",
   "metadata": {},
   "source": [
    "You can also use this to get something like the longest word in our set of words that we defined a moment ago. The key is just to define the correct function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa26be9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordLengthReducer: (leftWord: String, rightWord: String)String\n",
       "res50: String = Processing\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "def wordLengthReducer(leftWord:String, rightWord:String): String = {\n",
    "  if (leftWord.length > rightWord.length)\n",
    "    return leftWord\n",
    "  else\n",
    "    return rightWord\n",
    "}\n",
    "\n",
    "words.reduce(wordLengthReducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44560dda",
   "metadata": {},
   "source": [
    "This reducer is a good example because you can get one of two outputs. Because the reduce operation on the partitions is not deterministic, you can have either “definitive” or “processing” (both of length 10) as the “left” word. This means that sometimes you can end up with one, whereas other times you end up with the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88df5f4b",
   "metadata": {},
   "source": [
    "### count\n",
    "This method is fairly self-explanatory. Using it, you could, for example, count the number of rows in the RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13e66838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res51: Long = 17\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6920ad4",
   "metadata": {},
   "source": [
    "### countByValue\n",
    "This method counts the number of values in a given RDD. However, it does so by finally loading the result set into the memory of the driver. You should use this method only if the resulting map is expected to be small because the entire thing is loaded into the driver’s memory. Thus, this method makes sense only in a scenario in which either the total number of rows is low or the number of distinct items is low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e85674dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res52: scala.collection.Map[String,Long] = Map(Definitive -> 1, is -> 1, Processing -> 1, that -> 1, The -> 1, yeah -> 1, Spark -> 1, Made -> 1, Guide -> 1, Big -> 1, : -> 1, Simple, -> 1, truth -> 1, about -> 1, Data -> 2, the -> 1)\n"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9c91f",
   "metadata": {},
   "source": [
    "### first\n",
    "The `first` method returns the first value in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f60da65c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res53: String = Spark\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced0700",
   "metadata": {},
   "source": [
    "### max and min\n",
    "max and min return the maximum values and minimum values, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0150de3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res56: Int = 20\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(1 to 20).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e882c323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res55: Int = 1\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(1 to 20).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99ebd4",
   "metadata": {},
   "source": [
    "### take\n",
    "`take` and its derivative methods take a number of values from your RDD. This works by first scanning one partition and then using the results from that partition to estimate the number of additional partitions needed to satisfy the limit.\n",
    "\n",
    "There are many variations on this function, such as `takeOrdered, takeSample, and top`. You can use `takeSample` to specify a fixed-size random sample from your RDD. You can specify whether this should be done by using `withReplacement`, the number of values, as well as the random seed. top is effectively the opposite of `takeOrdered` in that it selects the top values according to the implicit ordering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c698b58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "withReplacement: Boolean = false\n",
       "numberToTake: Int = 6\n",
       "randomSeed: Long = 100\n",
       "res58: Array[String] = Array(Processing, Simple,, Spark, that, Data, Data)\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(5)\n",
    "words.takeOrdered(5)\n",
    "words.top(5)\n",
    "val withReplacement = false //true\n",
    "val numberToTake = 6\n",
    "val randomSeed = 100L\n",
    "words.takeSample(withReplacement, numberToTake, randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be41bde2",
   "metadata": {},
   "source": [
    "## Saving Files\n",
    "Saving files means writing to plain-text files. With RDDs, you cannot actually “save” to a data source in the conventional sense. You must iterate over the partitions in order to save the contents of each partition to some external database. This is a low-level approach that reveals the underlying operation that is being performed in the higher-level APIs. Spark will take each partition, and write that out to the destination.\n",
    "\n",
    "### saveAsTextFile\n",
    "To save to a text file, you just specify a path and optionally a compression codec:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4293aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.saveAsTextFile(\"file:/tmp/bookTitle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4828e",
   "metadata": {},
   "source": [
    "To set a compression codec, we must import the proper codec from Hadoop. You can find these in the `org.apache.hadoop.io.compress` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "749a77ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.hadoop.io.compress.BZip2Codec\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "import org.apache.hadoop.io.compress.BZip2Codec\n",
    "words.saveAsTextFile(\"file:/tmp/bookTitleCompressed\", classOf[BZip2Codec])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18adafd",
   "metadata": {},
   "source": [
    "### SequenceFiles\n",
    "Spark originally grew out of the Hadoop ecosystem, so it has a fairly tight integration with a variety of Hadoop tools. A `sequenceFile` is a flat file consisting of `binary key–value` pairs. It is extensively used in `MapReduce as input/output formats`.\n",
    "\n",
    "Spark can write to sequenceFiles using the `saveAsObjectFile` method or by explicitly writing key–value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "acc36336",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.saveAsObjectFile(\"/tmp/my/sequenceFilePath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12542d2",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "One feature not available in the DataFrame API is the concept of checkpointing. Checkpointing is the act of saving an RDD to disk so that future references to this RDD point to those intermediate partitions on disk rather than recomputing the RDD from its original source. This is similar to caching except that it’s not stored in memory, only disk. This can be helpful when performing iterative computation, similar to the use cases for caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setCheckpointDir(\"/some/path/for/checkpointing\")\n",
    "words.checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce16dc14",
   "metadata": {},
   "source": [
    "Now, when we reference this RDD, it will derive from the checkpoint instead of the source data. This can be a helpful optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e300fcc",
   "metadata": {},
   "source": [
    "## Pipe RDDs to System Commands\n",
    "The pipe method is probably one of Spark’s more interesting methods. With pipe, you can return an RDD created by piping elements to a forked external process. The resulting RDD is computed by executing the given process once per partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d0ccac89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res62: Array[String] = Array(\"       8\", \"       9\")\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.pipe(\"wc -l\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56313dec",
   "metadata": {},
   "source": [
    "### mapPartitions\n",
    "The previous command revealed that Spark operates on a per-partition basis when it comes to actually executing code. \n",
    "\n",
    "You also might have noticed earlier that the return signature of a map function on an RDD is actually MapPartitionsRDD. This is because map is just a row-wise alias for mapPartitions, which makes it possible for you to map an individual partition (represented as an iterator). \n",
    "\n",
    "\n",
    "That’s because physically on the cluster we operate on each partition individually (and not a specific row). A simple example creates the value “1” for every partition in our data, and the sum of the following expression will count the number of partitions we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bdd63e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res63: Double = 2.0\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "words.mapPartitions(part => Iterator[Int](1)).sum() // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae3ecd9",
   "metadata": {},
   "source": [
    "Other functions similar to mapPartitions include mapPartitionsWithIndex. With this you specify a function that accepts an index (within the partition) and an iterator that goes through all items within the partition. The partition index is the partition number in your RDD, which identifies where each record in our dataset sits (and potentially allows you to debug). You might use this to test whether your map functions are behaving correctly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2257ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexedFunc: (partitionIndex: Int, withinPartIterator: Iterator[String])Iterator[String]\n",
       "res64: Array[String] = Array(Partition: 0 => Spark, Partition: 0 => The, Partition: 0 => Definitive, Partition: 0 => Guide, Partition: 0 => :, Partition: 0 => Big, Partition: 0 => Data, Partition: 0 => Processing, Partition: 1 => Made, Partition: 1 => Simple,, Partition: 1 => yeah, Partition: 1 => that, Partition: 1 => is, Partition: 1 => the, Partition: 1 => truth, Partition: 1 => about, Partition: 1 => Data)\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "def indexedFunc(partitionIndex:Int, withinPartIterator: Iterator[String]) = {\n",
    "  withinPartIterator.toList.map(\n",
    "    value => s\"Partition: $partitionIndex => $value\").iterator\n",
    "}\n",
    "words.mapPartitionsWithIndex(indexedFunc).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b736f95d",
   "metadata": {},
   "source": [
    "## foreachPartition\n",
    "Although mapPartitions needs a return value to work properly, this next function does not. foreachPartition simply iterates over all the partitions of the data. The difference is that the function has no return value. This makes it great for doing something with each partition like writing it out to a database. In fact, this is how many data source connectors are written. You can create our own text file source if you want by specifying outputs to the temp directory with a random ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f6b3bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "words.foreachPartition { iter =>\n",
    "  import java.io._\n",
    "  import scala.util.Random\n",
    "  val randomFileName = new Random().nextInt()\n",
    "  val pw = new PrintWriter(new File(s\"/tmp/random-file-${randomFileName}.txt\"))\n",
    "  while (iter.hasNext) {\n",
    "      pw.write(iter.next())\n",
    "  }\n",
    "  pw.close()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20047aeb",
   "metadata": {},
   "source": [
    "## glom\n",
    "glom is an interesting function that takes every partition in your dataset and converts them to arrays. This can be useful if you’re going to collect the data to the driver and want to have an array for each partition. However, this can cause serious stability issues because if you have large partitions or a large number of partitions, it’s simple to crash the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0fcb9418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res66: Array[Array[String]] = Array(Array(Hello), Array(World))\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// in Scala\n",
    "spark.sparkContext.parallelize(Seq(\"Hello\", \"World\"), 2).glom().collect()\n",
    "// Array(Array(Hello), Array(World))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc431b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33089c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
